\newcommand{\authorinfotitle}{Vanessa Closius, Jonas Tietz, Tronje Krabbe}
\newcommand{\authorinfo}{Vanessa Closius, Jonas Tietz, Tronje Krabbe}
\newcommand{\titleinfo}{MMS}
\newcommand{\qed}{\square}

\documentclass[a4paper,11pt]{article}
%\usepackage[german,ngerman]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{breqn}
\usepackage{fancyhdr}
\usepackage{multicol}
\usepackage{tikz}
\usepackage{trfsigns}
\usepackage{float}

\author{\authorinfotitle}
\title{\titleinfo}
\date{\today}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{\authorinfo}
\fancyhead[L]{MMS Hausaufgaben}
\fancyfoot[C]{\thepage}
\allowdisplaybreaks
\begin{document}
	\maketitle
	\begin{enumerate}
		% Aufgabe 1
		\item[\textbf{1.}]
		
		\begin{enumerate}
		% 1.a
		\item[\textbf{a)}]
		For $x \geq 0$ the MLE can be calculated by taking the logarithm of $p(x|\beta)$ and then using the first derivative to calculate the maximum:
		\begin{align*}
		\hat{\beta}^{ML} &= argmax_\beta p_{x|\beta}(x|\beta) = \prod_{n=1}^{N}\frac{1}{\beta}\exp(-\frac{x}{\beta}) \\
		&= argmax_\beta \log p_{x|\beta}(x|\beta) = \sum_{n=1}^{N} (-\log(\beta)-\frac{x}{\beta}) \\
		\frac{\partial \log p(x|\beta)}{\partial \beta} &= \sum_{n=1}^{N} (-\frac{1}{\beta}+\frac{x}{\beta^2}) \underbrace{=}_{\text{MLE Maximum}} 0 \\
		&-N + \frac{1}{\beta} \sum_{n=1}^{N} x = 0 \\
		\Rightarrow \hat{\beta}^{ML} &= \frac{1}{N} \sum_{n=1}^{N} x
		\end{align*}
			% 1.b
		\item[\textbf{b)}]
		The resulting estimator is biased:
		\begin{align*}
		E(\hat{\beta}^{ML}) &= E(\frac{1}{N} \sum_{n=1}^{N} x) = \frac{1}{N} \sum_{n=1}^{N} E(x) = \frac{1}{N} \sum_{n=1}^{N} \beta = \frac{1}{N} \cdot N\beta = \beta
		\end{align*}
		\end{enumerate}

	\end{enumerate}
\end{document}
